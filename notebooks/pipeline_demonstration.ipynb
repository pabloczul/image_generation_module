{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation Pipeline Demonstration\n",
    "\n",
    "This notebook provides a comprehensive step-by-step demonstration of the image generation pipeline capabilities. We'll showcase various features including quality assessment, segmentation, mask refinement, and background replacement with different techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Initialization\n",
    "\n",
    "First, we'll import the necessary components and initialize the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Add the parent directory to the path to allow importing from src\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"Added {project_root} to sys.path\")\n",
    "\n",
    "# Import pipeline components\n",
    "from src.pipeline.main_pipeline import GenerationPipeline\n",
    "from src.image.quality import ImageAssessor\n",
    "from src.image.segmentation import Segmenter\n",
    "from src.image.filtering import refine_morphological\n",
    "from src.utils.data_io import load_image, save_image\n",
    "from src.background.generators import generate_solid_background, generate_gradient_background\n",
    "from src.background.utils import combine_foreground_background\n",
    "from src.models.diffusion import DiffusionGenerator\n",
    "from src.config import DEFAULT_CONFIG\n",
    "\n",
    "# Set up display utilities\n",
    "def display_images(images, titles=None, figsize=(15, 10), rows=1):\n",
    "    \"\"\"Display multiple images in a row with titles.\"\"\"\n",
    "    cols = len(images) // rows if len(images) % rows == 0 else len(images) // rows + 1\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        if isinstance(image, np.ndarray):\n",
    "            if image.ndim == 2 or (image.ndim == 3 and image.shape[2] == 1):\n",
    "                # Grayscale image\n",
    "                plt.imshow(image, cmap='gray')\n",
    "            else:\n",
    "                # Assume RGB order for numpy arrays\n",
    "                plt.imshow(image)\n",
    "        else:\n",
    "            # Assume PIL image\n",
    "            plt.imshow(np.array(image))\n",
    "        if titles and i < len(titles):\n",
    "            plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Path to the demo image\n",
    "IMAGE_PATH = project_root / \"data\" / \"images\" / \"961939-MLC77038553924_062024.jpg\"\n",
    "\n",
    "# Create results directory for intermediate outputs\n",
    "RESULTS_DIR = project_root / \"results\" / \"demo_outputs\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "if cuda_available:\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "# Initialize the pipeline\n",
    "try:\n",
    "    # Enable diffusion for the demonstration\n",
    "    pipeline = GenerationPipeline(diffusion_enabled=True)\n",
    "    print(\"Pipeline initialized successfully.\")\n",
    "    print(f\"Segmentation model: {pipeline.segmenter.model_name}\")\n",
    "    print(f\"Diffusion enabled: {pipeline.diffusion_generator is not None}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing pipeline: {e}\")\n",
    "    # Fallback to non-diffusion pipeline if needed\n",
    "    print(\"Falling back to non-diffusion pipeline...\")\n",
    "    pipeline = GenerationPipeline(diffusion_enabled=False)\n",
    "\n",
    "# Load the demo image\n",
    "try:\n",
    "    demo_image_pil = load_image(IMAGE_PATH)\n",
    "    demo_image_np = np.array(demo_image_pil)\n",
    "    print(f\"Loaded demo image: {IMAGE_PATH.name}\")\n",
    "    print(f\"Image dimensions: {demo_image_pil.size[0]}x{demo_image_pil.size[1]}\")\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(demo_image_pil)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading demo image: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Quality Assessment\n",
    "\n",
    "Let's evaluate the quality of the input image to determine if it's suitable for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess image quality\n",
    "assessor = pipeline.assessor\n",
    "quality_results = assessor.assess_quality(demo_image_pil)\n",
    "\n",
    "print(\"Quality Assessment Results:\")\n",
    "print(f\"Resolution: {quality_results['resolution']['width']}x{quality_results['resolution']['height']}\")\n",
    "print(f\"Resolution check: {'Passed' if quality_results['resolution']['passed'] else 'Failed'}\")\n",
    "print(f\"Blur value: {quality_results['blur']['value']}\")\n",
    "print(f\"Blur check: {'Passed' if quality_results['blur']['passed'] else 'Failed'}\")\n",
    "print(f\"Contrast value: {quality_results['contrast']['value']}\")\n",
    "print(f\"Contrast check: {'Passed' if quality_results['contrast']['passed'] else 'Failed'}\")\n",
    "print(f\"\\nOverall quality: {quality_results['overall_quality']}\")\n",
    "print(f\"Is processable: {quality_results['is_processable']}\")\n",
    "print(f\"Message: {quality_results['message']}\")\n",
    "\n",
    "# Get processing strategy recommendation\n",
    "strategy = assessor.recommend_processing_strategy(quality_results)\n",
    "print(\"\\nRecommended Processing Strategy:\")\n",
    "print(f\"Suggested segmentation model: {strategy['segmentation_model']}\")\n",
    "print(f\"Mask refinement operations: {strategy['mask_refinement_ops']}\")\n",
    "print(f\"Suggested background type: {strategy['suggested_background_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Segmentation Process\n",
    "\n",
    "Now we'll segment the image to isolate the foreground object from the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the segmenter instance from the pipeline\n",
    "segmenter = pipeline.segmenter\n",
    "print(f\"Using segmentation model: {segmenter.model_name}\")\n",
    "\n",
    "# Segment the image\n",
    "start_time = time.time()\n",
    "raw_mask = segmenter.segment(demo_image_np, \n",
    "                             save_intermediate=True, \n",
    "                             intermediate_dir=RESULTS_DIR, \n",
    "                             output_basename=\"demo\")\n",
    "segmentation_time = time.time() - start_time\n",
    "print(f\"Segmentation completed in {segmentation_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate mask quality\n",
    "mask_quality = segmenter.evaluate_mask_quality(raw_mask)\n",
    "print(\"Mask Quality Metrics:\")\n",
    "print(f\"Coverage: {mask_quality['coverage']}%\")\n",
    "print(f\"Complexity: {mask_quality['complexity']}\")\n",
    "print(f\"Contours: {mask_quality['contours']}\")\n",
    "print(f\"Is valid: {mask_quality['is_valid']}\")\n",
    "\n",
    "# Visualize the raw mask\n",
    "display_images([demo_image_pil, raw_mask], \n",
    "               titles=[\"Original Image\", \"Raw Segmentation Mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mask Refinement and Filtering\n",
    "\n",
    "Let's refine the raw mask and apply various filters to improve the quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the default config\n",
    "refinement_config = DEFAULT_CONFIG.copy()\n",
    "\n",
    "# Define parameters for different refinement operations\n",
    "opening_sizes = [0, 5]  # 0 means disabled, 5 is kernel size\n",
    "closing_sizes = [7, 15]  # Kernel sizes for closing operation\n",
    "\n",
    "# Store masks for visualization\n",
    "refined_masks = [raw_mask]  # Start with the raw mask\n",
    "mask_titles = [\"Raw Mask\"]\n",
    "\n",
    "# Apply different morphological operations\n",
    "for opening_size in opening_sizes:\n",
    "    for closing_size in closing_sizes:\n",
    "        # Update config\n",
    "        refinement_config[\"mask_opening_kernel_size\"] = opening_size\n",
    "        refinement_config[\"mask_opening_iterations\"] = 1\n",
    "        refinement_config[\"mask_closing_kernel_size\"] = closing_size\n",
    "        refinement_config[\"mask_closing_iterations\"] = 1\n",
    "        \n",
    "        # Refine mask\n",
    "        refined_mask = refine_morphological(raw_mask.copy(), refinement_config)\n",
    "        \n",
    "        # Evaluate refined mask quality\n",
    "        refined_quality = segmenter.evaluate_mask_quality(refined_mask)\n",
    "        \n",
    "        # Add to display list\n",
    "        refined_masks.append(refined_mask)\n",
    "        mask_titles.append(f\"Open={opening_size}, Close={closing_size}\\nCoverage={refined_quality['coverage']:.1f}%\")\n",
    "        \n",
    "        # Save the refined mask\n",
    "        refined_path = RESULTS_DIR / f\"demo_refined_o{opening_size}_c{closing_size}.png\"\n",
    "        save_image(Image.fromarray(refined_mask), refined_path)\n",
    "        \n",
    "# Display the masks\n",
    "display_images(refined_masks, mask_titles, figsize=(20, 15), rows=2)\n",
    "\n",
    "# Choose the best refined mask based on the results\n",
    "best_config = refinement_config.copy()\n",
    "best_config[\"mask_opening_kernel_size\"] = 5\n",
    "best_config[\"mask_closing_kernel_size\"] = 15\n",
    "best_mask = refine_morphological(raw_mask.copy(), best_config)\n",
    "\n",
    "# Apply edge feathering (for smoother edges)\n",
    "feather_amount = 2.0  # Sigma for gaussian blur\n",
    "if feather_amount > 0:\n",
    "    mask_float = best_mask.astype(np.float32) / 255.0\n",
    "    k_size = int(6 * feather_amount + 1)\n",
    "    if k_size % 2 == 0: k_size += 1\n",
    "    feathered_mask_float = cv2.GaussianBlur(mask_float, (k_size, k_size), feather_amount)\n",
    "    feathered_mask = (feathered_mask_float * 255).clip(0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Display feathering effect\n",
    "    display_images([best_mask, feathered_mask], \n",
    "                 titles=[\"Best Refined Mask\", f\"Feathered Mask (sigma={feather_amount})\"])\n",
    "else:\n",
    "    feathered_mask = best_mask\n",
    "    print(\"Feathering skipped (amount = 0)\")\n",
    "\n",
    "# Save the feathered mask\n",
    "save_image(Image.fromarray(feathered_mask), RESULTS_DIR / \"demo_mask_feathered.png\")\n",
    "\n",
    "# Create RGBA foreground (using final mask)\n",
    "foreground_rgba_np = cv2.cvtColor(demo_image_np, cv2.COLOR_RGB2RGBA)\n",
    "foreground_rgba_np[:, :, 3] = feathered_mask\n",
    "\n",
    "# Visualize the foreground with transparency\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(foreground_rgba_np)\n",
    "plt.title(\"Foreground with Transparency\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Background Replacement - Solid Color\n",
    "\n",
    "Now let's replace the background with solid colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions for background generation\n",
    "width, height = demo_image_pil.size\n",
    "\n",
    "# Generate white background\n",
    "white_bg = generate_solid_background(width, height, color=(255, 255, 255))\n",
    "white_bg_np = np.array(white_bg)\n",
    "\n",
    "# Generate colored background (light blue)\n",
    "light_blue_bg = generate_solid_background(width, height, color=(235, 245, 255))\n",
    "light_blue_bg_np = np.array(light_blue_bg)\n",
    "\n",
    "# Combine foreground with backgrounds\n",
    "white_result = combine_foreground_background(foreground_rgba_np, white_bg_np)\n",
    "blue_result = combine_foreground_background(foreground_rgba_np, light_blue_bg_np)\n",
    "\n",
    "# Add shadow (optional)\n",
    "# Create shadow layer with offset\n",
    "def add_shadow(foreground_rgba, background):\n",
    "    # Extract alpha channel for shadow generation\n",
    "    alpha = foreground_rgba[:, :, 3]\n",
    "    h, w = alpha.shape\n",
    "    \n",
    "    # Create shadow mask with blur\n",
    "    shadow_mask = (alpha > 10).astype(np.uint8) * 255\n",
    "    blur_sigma = 5.0\n",
    "    k_size = int(6 * blur_sigma + 1)\n",
    "    if k_size % 2 == 0: k_size += 1\n",
    "    shadow_mask = cv2.GaussianBlur(shadow_mask, (k_size, k_size), blur_sigma)\n",
    "    \n",
    "    # Create shadow layer with opacity\n",
    "    shadow_layer = np.zeros((h, w, 4), dtype=np.uint8)\n",
    "    shadow_color = (0, 0, 0)  # Black shadow\n",
    "    shadow_layer[:, :, 0] = shadow_color[0]\n",
    "    shadow_layer[:, :, 1] = shadow_color[1]\n",
    "    shadow_layer[:, :, 2] = shadow_color[2]\n",
    "    shadow_layer[:, :, 3] = (shadow_mask * 0.5).astype(np.uint8)  # 50% opacity\n",
    "    \n",
    "    # Shift shadow\n",
    "    x_off, y_off = 10, 10  # Shadow offset\n",
    "    M = np.float32([[1, 0, x_off], [0, 1, y_off]])\n",
    "    shifted_shadow = cv2.warpAffine(shadow_layer, M, (w, h))\n",
    "    \n",
    "    # Convert to PIL for easier compositing\n",
    "    shadow_pil = Image.fromarray(shifted_shadow)\n",
    "    bg_pil = Image.fromarray(background).convert('RGBA')\n",
    "    fg_pil = Image.fromarray(foreground_rgba)\n",
    "    \n",
    "    # Composite shadow onto background first, then foreground\n",
    "    result = bg_pil.copy()\n",
    "    result.paste(shadow_pil, (0, 0), shadow_pil)\n",
    "    result.paste(fg_pil, (0, 0), fg_pil)\n",
    "    \n",
    "    return np.array(result.convert('RGB'))\n",
    "\n",
    "# Add shadows to both backgrounds\n",
    "white_result_with_shadow = add_shadow(foreground_rgba_np, white_bg_np)\n",
    "blue_result_with_shadow = add_shadow(foreground_rgba_np, light_blue_bg_np)\n",
    "\n",
    "# Show results\n",
    "display_images([white_result, white_result_with_shadow, blue_result, blue_result_with_shadow],\n",
    "               titles=[\"White Background\", \"White Background + Shadow\", \n",
    "                       \"Light Blue Background\", \"Light Blue Background + Shadow\"],\n",
    "               figsize=(20, 15), rows=2)\n",
    "\n",
    "# Save results\n",
    "save_image(Image.fromarray(white_result_with_shadow), RESULTS_DIR / \"demo_white_bg.png\")\n",
    "save_image(Image.fromarray(blue_result_with_shadow), RESULTS_DIR / \"demo_blue_bg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Background Replacement - Gradient\n",
    "\n",
    "Let's create gradient backgrounds and use them for composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate vertical gradient\n",
    "vertical_gradient = generate_gradient_background(\n",
    "    width, height, \n",
    "    colors=[(240, 240, 245), (220, 220, 235)], \n",
    "    direction='vertical'\n",
    ")\n",
    "vertical_gradient_np = np.array(vertical_gradient)\n",
    "\n",
    "# Generate horizontal gradient\n",
    "horizontal_gradient = generate_gradient_background(\n",
    "    width, height, \n",
    "    colors=[(240, 245, 255), (220, 235, 255)], \n",
    "    direction='horizontal'\n",
    ")\n",
    "horizontal_gradient_np = np.array(horizontal_gradient)\n",
    "\n",
    "# Generate radial gradient\n",
    "radial_gradient = generate_gradient_background(\n",
    "    width, height, \n",
    "    colors=[(245, 240, 235), (225, 220, 215)], \n",
    "    direction='radial'\n",
    ")\n",
    "radial_gradient_np = np.array(radial_gradient)\n",
    "\n",
    "# Show the gradients\n",
    "display_images([vertical_gradient, horizontal_gradient, radial_gradient],\n",
    "               titles=[\"Vertical Gradient\", \"Horizontal Gradient\", \"Radial Gradient\"])\n",
    "\n",
    "# Combine foreground with gradients and add shadows\n",
    "vertical_result = add_shadow(foreground_rgba_np, vertical_gradient_np)\n",
    "horizontal_result = add_shadow(foreground_rgba_np, horizontal_gradient_np)\n",
    "radial_result = add_shadow(foreground_rgba_np, radial_gradient_np)\n",
    "\n",
    "# Show the results\n",
    "display_images([vertical_result, horizontal_result, radial_result],\n",
    "               titles=[\"Vertical Gradient Background\", \"Horizontal Gradient Background\", \"Radial Gradient Background\"])\n",
    "\n",
    "# Save results\n",
    "save_image(Image.fromarray(vertical_result), RESULTS_DIR / \"demo_vertical_gradient.png\")\n",
    "save_image(Image.fromarray(horizontal_result), RESULTS_DIR / \"demo_horizontal_gradient.png\")\n",
    "save_image(Image.fromarray(radial_result), RESULTS_DIR / \"demo_radial_gradient.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Background Replacement - Diffusion\n",
    "\n",
    "Finally, let's use Stable Diffusion with ControlNet to generate custom backgrounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the diffusion generator is available\n",
    "if pipeline.diffusion_generator is None:\n",
    "    print(\"Diffusion generator is not available. Skipping this section.\")\n",
    "else:\n",
    "    # Define prompts for background generation\n",
    "    prompts = [\n",
    "        \"Minimalist studio setting with soft lighting, clean white background\",\n",
    "        \"Professional product photography studio with subtle shadow and clean background\",\n",
    "        \"Elegant soft gradient background, minimalist product shot\"\n",
    "    ]\n",
    "    \n",
    "    # Set generation parameters\n",
    "    diffusion_params = {\n",
    "        \"num_inference_steps\": 25,\n",
    "        \"guidance_scale\": 7.5,\n",
    "        \"controlnet_conditioning_scale\": 0.7,\n",
    "        \"negative_prompt\": \"low quality, bad quality, blurry, distorted, text, watermark\"\n",
    "    }\n",
    "    \n",
    "    # Generate backgrounds with different prompts\n",
    "    diffusion_results = []\n",
    "    diffusion_titles = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"Generating background with prompt: '{prompt}'\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert mask to PIL\n",
    "        final_mask_pil = Image.fromarray(feathered_mask).convert('L')\n",
    "        \n",
    "        # Generate background\n",
    "        try:\n",
    "            generated_bg = pipeline.diffusion_generator.generate(\n",
    "                image_input=demo_image_pil,\n",
    "                foreground_mask=final_mask_pil,\n",
    "                prompt=prompt,\n",
    "                **diffusion_params\n",
    "            )\n",
    "            generation_time = time.time() - start_time\n",
    "            print(f\"Background generation completed in {generation_time:.2f} seconds\")\n",
    "            \n",
    "            # Save the generated background\n",
    "            save_image(generated_bg, RESULTS_DIR / f\"demo_diffusion_bg_{i+1}.png\")\n",
    "            \n",
    "            # Combine foreground with the generated background\n",
    "            generated_bg_np = np.array(generated_bg)\n",
    "            combined_result = add_shadow(foreground_rgba_np, generated_bg_np)\n",
    "            \n",
    "            # Save the combined result\n",
    "            save_image(Image.fromarray(combined_result), RESULTS_DIR / f\"demo_diffusion_result_{i+1}.png\")\n",
    "            \n",
    "            # Store for display\n",
    "            diffusion_results.append(combined_result)\n",
    "            diffusion_titles.append(f\"Prompt {i+1}: {prompt[:40]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating background: {e}\")\n",
    "    \n",
    "    # Display the results\n",
    "    if diffusion_results:\n",
    "        display_images(diffusion_results, diffusion_titles, figsize=(20, 10), rows=len(diffusion_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Performance Analysis\n",
    "\n",
    "Let's compare the different approaches and analyze the performance considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the generated results for comparison\n",
    "final_results = []\n",
    "final_titles = []\n",
    "\n",
    "# Add original image\n",
    "final_results.append(demo_image_np)\n",
    "final_titles.append(\"Original Image\")\n",
    "\n",
    "# Add solid background results\n",
    "final_results.append(white_result_with_shadow)\n",
    "final_titles.append(\"Solid White Background\")\n",
    "\n",
    "# Add gradient result\n",
    "final_results.append(vertical_result)\n",
    "final_titles.append(\"Vertical Gradient\")\n",
    "\n",
    "# Add diffusion result if available\n",
    "if 'diffusion_results' in locals() and diffusion_results:\n",
    "    final_results.append(diffusion_results[0])\n",
    "    final_titles.append(\"Diffusion Background\")\n",
    "\n",
    "# Display final comparison\n",
    "display_images(final_results, final_titles, figsize=(20, 15), rows=2)\n",
    "\n",
    "# Performance discussion\n",
    "print(\"Performance Analysis:\")\n",
    "print(\"\\n1. Computational Requirements:\")\n",
    "print(\"   - Solid Color Backgrounds: Very low computation, instant generation\")\n",
    "print(\"   - Gradient Backgrounds: Low computation, nearly instant generation\")\n",
    "print(\"   - Diffusion Backgrounds: High computation, requires GPU for reasonable speed\")\n",
    "\n",
    "print(\"\\n2. Quality Considerations:\")\n",
    "print(\"   - Solid Color Backgrounds: Clean, professional but simple\")\n",
    "print(\"   - Gradient Backgrounds: More visual interest while maintaining simplicity\")\n",
    "print(\"   - Diffusion Backgrounds: High customization but may introduce artifacts\")\n",
    "\n",
    "print(\"\\n3. Use Case Recommendations:\")\n",
    "print(\"   - For e-commerce catalog bulk processing: Use solid colors or gradients\")\n",
    "print(\"   - For featured products: Consider diffusion for unique, custom backgrounds\")\n",
    "print(\"   - For maximum quality control: Solid colors or gradients ensure consistency\")\n",
    "\n",
    "print(\"\\n4. Pipeline Efficiency:\")\n",
    "print(\"   - Segmentation is a bottleneck for all methods\")\n",
    "print(\"   - Mask refinement significantly improves quality with minimal cost\")\n",
    "print(\"   - Diffusion generation is the most expensive step when enabled\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 